```
let me try it out. Thank you for that. I really appreciate the help.
I agree that the *from future* is quite useful for all of the people who use Python 3 already, but I'm less convinced of the *with open() as f* context manager. I think the file object will be closed anyway when it goes out of scope; the context manager seems to add extra complexity and an extra indentation level. However I _do_ think a file context is useful when writing a file as part of a larger operation, so you can indeed force it to close (and flush to disk).
I did try python 3 earlier but a lot of the scripts/recipes were written in 2.7, so I had to downgrade
Uh-uh. I see.
Well, it's not forbidden to use Python 2 ;-)
I used it for 25 years and I'm still using it.
i understand :slightly_smiling_face: Apologies for being a bit slow here
the script worked
Great! And now?
i can run that in the command prompt and save the output.
would i be able to edit the code and add some more to it? as in to get other data from it?
What is your ultimate goal? To extract the tweets from the file?
Oh yeah sure you could extract anything and everything from it
Basically I just need to make the file easier to read so I can count the number of tweets, retweets, see who tweets the most, and do a word cloud.
<@U69HXQZJ9> Ah, you want to do some statistics on the data.
Yup. That's what the plan is. I just need to get a better idea of what people are doing and saying
Well, counting the tweets is not hard: that's just the number of tweet objects in your stream file.
yeah i mean that was just the easiest part :slightly_smiling_face:
there was another script which had the functions i needed but it gave me the same error
Counting the number of retweets is somewhat harder, is that mentioned in the Tweet object? If so, you could fetch it and do something with it. Retweets always belong to one specific tweet, no?
well i could skip that. just would prefer to see who the most prolific ones are
You mean, you are only interested in tweets that get a lot of retweets?
well i am analysing twitter trends. have four big files. want to see who tweets the most, which ones got the most RTs, geotag them, and make a word cloud.
Why do you have 4 big files, and not 2 or 6 or 35?
the script i followed didn't give me the option to make smaller ones
so basically i collected a lot of tweets for trends
the four files are for four trends
Ah, every trend gets its own stream file!
yes
and those files are around 1 gb to 1.5 gb each
*who tweets the most* -&gt; Every tweet has a 'user' object, and in there is a 'name' attribute with the name of the user who made the tweet. You could use a dictionary to store all usernames and the tweet count. You know, going from the beginning to the end of the tweets and storing the username in a dict, or incrementing the tweet count if the user is already in the dict.
The when the counting is done, you could convert the dict to a list of (name, tweetcount) tuples and sort that on decreasing tweetcount. Then print out the first 10 items of the result and you have a TOP 10 of most prolific tweeters.
Or is it twitterers?
:stuck_out_tongue_winking_eye:
Either name works :)
<@U69HXQZJ9> Dont worry about the file size because the construction 'for line in f: ...' doesn't load the entire file into memory.
Ahh
So no need to iterate 
Would I be too lazy if I asked someone here how I can do that? 
